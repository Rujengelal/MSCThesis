{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rujengelal/MSCThesis/blob/main/MSCThesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re_MNgFGHZXn"
      },
      "source": [
        "### Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7hu8-OV1HZXo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q flwr[simulation] flwr-datasets[vision] torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lQeAT6nmHZXp",
        "outputId": "09787d43-fca1-4339-b8dd-245780c3e181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda\n",
            "Flower 1.19.0 / PyTorch 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import flwr\n",
        "from flwr.client import Client, ClientApp, NumPyClient\n",
        "from flwr.common import Context\n",
        "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
        "from flwr.server.strategy import Strategy\n",
        "from flwr.simulation import run_simulation\n",
        "from flwr_datasets import FederatedDataset\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Try \"cuda\" to train on GPU\n",
        "print(f\"Training on {DEVICE}\")\n",
        "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQEUwEyAHZXp"
      },
      "source": [
        "It is possible to switch to a runtime that has GPU acceleration enabled (on Google Colab: `Runtime > Change runtime type > Hardware acclerator: GPU > Save`). Note, however, that Google Colab is not always able to offer GPU acceleration. If you see an error related to GPU availability in one of the following sections, consider switching back to CPU-based execution by setting `DEVICE = torch.device(\"cpu\")`. If the runtime has GPU acceleration enabled, you should see the output `Training on cuda`, otherwise it'll say `Training on cpu`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWub3-HRHZXq"
      },
      "source": [
        "### Data loading\n",
        "\n",
        "Let's now load the CIFAR-10 training and test set, partition them into ten smaller datasets (each split into training and validation set), and wrap everything in their own `DataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "InVRFqQ2HZXq"
      },
      "outputs": [],
      "source": [
        "def load_datasets(partition_id, num_partitions: int):\n",
        "    fds = FederatedDataset(dataset=\"uoft-cs/cifar10\", partitioners={\"train\": num_partitions})\n",
        "    partition = fds.load_partition(partition_id)\n",
        "    # Divide data on each node: 80% train, 20% test\n",
        "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
        "    pytorch_transforms = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "    )\n",
        "\n",
        "    def apply_transforms(batch):\n",
        "        # Instead of passing transforms to CIFAR10(..., transform=transform)\n",
        "        # we will use this function to dataset.with_transform(apply_transforms)\n",
        "        # The transforms object is exactly the same\n",
        "        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
        "        return batch\n",
        "\n",
        "    partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
        "    trainloader = DataLoader(partition_train_test[\"train\"], batch_size=32, shuffle=True)\n",
        "    valloader = DataLoader(partition_train_test[\"test\"], batch_size=32)\n",
        "    testset = fds.load_split(\"test\").with_transform(apply_transforms)\n",
        "    testloader = DataLoader(testset, batch_size=32)\n",
        "    return trainloader, valloader, testloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxyXuoHxHZXq"
      },
      "source": [
        "### Model training/evaluation\n",
        "\n",
        "Let's continue with the usual model definition (including `set_parameters` and `get_parameters`), training and test functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Kjg4TLRPHZXq"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_parameters(net) -> List[np.ndarray]:\n",
        "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
        "\n",
        "\n",
        "def set_parameters(net, parameters: List[np.ndarray]):\n",
        "    params_dict = zip(net.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "    net.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def train(net, trainloader, epochs: int):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters())\n",
        "    net.train()\n",
        "    for epoch in range(epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for batch in trainloader:\n",
        "            images, labels = batch[\"img\"], batch[\"label\"]\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = criterion(net(images), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Metrics\n",
        "            epoch_loss += loss\n",
        "            total += labels.size(0)\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "        epoch_loss /= len(trainloader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
        "\n",
        "\n",
        "def test(net, testloader):\n",
        "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in testloader:\n",
        "            images, labels = batch[\"img\"], batch[\"label\"]\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QImHmAFKHZXr"
      },
      "source": [
        "### Flower client\n",
        "\n",
        "To implement the Flower client, we (again) create a subclass of `flwr.client.NumPyClient` and implement the three methods `get_parameters`, `fit`, and `evaluate`. Here, we also pass the `partition_id` to the client and use it log additional details. We then create an instance of `ClientApp` and pass it the `client_fn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lP2M4rNRHZXr"
      },
      "outputs": [],
      "source": [
        "# class FlowerClient(NumPyClient):\n",
        "#     def __init__(self, partition_id, net, trainloader, valloader):\n",
        "#         self.partition_id = partition_id\n",
        "#         self.net = net\n",
        "#         self.trainloader = trainloader\n",
        "#         self.valloader = valloader\n",
        "\n",
        "#     def get_parameters(self, config):\n",
        "#         print(f\"[Client {self.partition_id}] get_parameters\")\n",
        "#         return get_parameters(self.net)\n",
        "\n",
        "#     def fit(self, parameters, config):\n",
        "#         print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
        "#         set_parameters(self.net, parameters)\n",
        "#         train(self.net, self.trainloader, epochs=1)\n",
        "#         return get_parameters(self.net), len(self.trainloader), {}\n",
        "\n",
        "#     def evaluate(self, parameters, config):\n",
        "#         print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
        "#         set_parameters(self.net, parameters)\n",
        "#         loss, accuracy = test(self.net, self.valloader)\n",
        "#         return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
        "\n",
        "\n",
        "# def client_fn(context: Context) -> Client:\n",
        "#     net = Net().to(DEVICE)\n",
        "#     partition_id = context.node_config[\"partition-id\"]\n",
        "#     num_partitions = context.node_config[\"num-partitions\"]\n",
        "#     trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
        "#     return FlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
        "\n",
        "\n",
        "# # Create the ClientApp\n",
        "# client = ClientApp(client_fn=client_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def distillation(teacher,student,data_loader,distillation_epochs: int,L,T):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    KLLoss=nn.KLDivLoss()\n",
        "    optimizer = torch.optim.Adam(student.parameters())\n",
        "    student.train()\n",
        "    for epoch in range(distillation_epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for batch in data_loader:\n",
        "            images, labels = batch[\"img\"], batch[\"label\"]\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            teacher_output=teacher(images)\n",
        "            teacher_soft_labels=F.softmax(teacher_output/T,dim=1)\n",
        "\n",
        "            student_output=student(images)\n",
        "            student_soft_labels=F.softmax(student_output/T,dim=1)\n",
        "\n",
        "\n",
        "            loss = KLLoss(student_soft_labels,teacher_soft_labels)*L*T*T +criterion(student_output, labels)*(1-L)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Metrics\n",
        "            epoch_loss += loss\n",
        "            total += labels.size(0)\n",
        "            correct += (torch.max(student_output.data, 1)[1] == labels).sum().item()\n",
        "        epoch_loss /= len(data_loader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        print(f\"Distillation Epoch {epoch+1}: T {T}, L {L}, loss {epoch_loss}, accuracy {epoch_acc}\")"
      ],
      "metadata": {
        "id": "lilMXlyFSNAS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cEzzLz9H8UVL"
      },
      "outputs": [],
      "source": [
        "class FlowerClient(NumPyClient):\n",
        "    def __init__(self, partition_id, net, trainloader, valloader):\n",
        "        self.partition_id = partition_id\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "        self.teacher_model=None\n",
        "\n",
        "        #Make directory for storing teacher models\n",
        "        #Load teacher model if it exists\n",
        "        teacher_models_dir=Path(\"teacher_models\")\n",
        "        teacher_models_dir.mkdir(parents=True,exist_ok=True)\n",
        "        self.teacher_model_path=teacher_models_dir/f\"teacher_model_{partition_id}.pt\"\n",
        "        if self.teacher_model_path.exists():\n",
        "          self.teacher_model=deepcopy(self.net)\n",
        "          self.teacher_model.load_state_dict(torch.load(self.teacher_model_path))\n",
        "          self.teacher_model.eval()\n",
        "          self.teacher_model.to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
        "        return get_parameters(self.net)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
        "        set_parameters(self.net, parameters)\n",
        "\n",
        "        #Set the teacher model based on the global model on first run\n",
        "        if self.teacher_model is None:\n",
        "          torch.save(self.net.state_dict(), self.teacher_model_path)\n",
        "\n",
        "        train(self.net, self.trainloader, epochs=1)\n",
        "\n",
        "        temperatures = [1, 5, 25]\n",
        "        imitation_parameters = [0, 0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "        for T in temperatures:\n",
        "          for L in imitation_parameters:\n",
        "            student=deepcopy(self.net).to(DEVICE)\n",
        "            distillation(self.teacher_model,student,self.trainloader,distillation_epochs=1,L=L,T=T)\n",
        "\n",
        "        return get_parameters(self.net), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
        "        set_parameters(self.net, parameters)\n",
        "        loss, accuracy = test(self.net, self.valloader)\n",
        "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
        "\n",
        "\n",
        "def client_fn(context: Context) -> Client:\n",
        "    net = Net().to(DEVICE)\n",
        "    partition_id = context.node_config[\"partition-id\"]\n",
        "    num_partitions = context.node_config[\"num-partitions\"]\n",
        "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
        "    return FlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
        "\n",
        "\n",
        "# Create the ClientApp\n",
        "client = ClientApp(client_fn=client_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZbsmrdQHZXr",
        "outputId": "d9896956-4ba9-49ec-d5e8-f88e471ec11e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:flwr:Asyncio event loop already running.\n",
            "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=3, no round_timeout\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "\u001b[36m(pid=7784)\u001b[0m 2025-07-15 19:08:09.233019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=7784)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=7784)\u001b[0m E0000 00:00:1752606489.274278    7784 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=7784)\u001b[0m E0000 00:00:1752606489.285473    7784 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m /usr/local/lib/python3.11/dist-packages/jupyter_client/connect.py:28: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m given by the platformdirs library.  To remove this warning and\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m see the appropriate new directories, set the environment variable\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m The use of platformdirs will be the default in `jupyter_core` v6\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m   from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write\n",
            "\u001b[36m(pid=7783)\u001b[0m 2025-07-15 19:08:09.230749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=7783)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=7783)\u001b[0m E0000 00:00:1752606489.268034    7783 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=7783)\u001b[0m E0000 00:00:1752606489.279307    7783 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
            "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 9] get_parameters\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 0] fit, config: {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m /usr/local/lib/python3.11/dist-packages/jupyter_client/connect.py:28: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m given by the platformdirs library.  To remove this warning and\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m see the appropriate new directories, set the environment variable\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m The use of platformdirs will be the default in `jupyter_core` v6\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m   from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.06583557277917862, accuracy 0.202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.0592544823884964, accuracy 0.29025\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.25, loss 0.042347636073827744, accuracy 0.30875\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 3] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.5, loss 0.025755517184734344, accuracy 0.29475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.06471900641918182, accuracy 0.22525\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.75, loss 0.009068425744771957, accuracy 0.309\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0, loss 0.0587412565946579, accuracy 0.30525\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.5, loss -0.06455055624246597, accuracy 0.303\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 5, L 0, loss 0.0561484657227993, accuracy 0.33575\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0, loss 0.05875492841005325, accuracy 0.2935\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.316849708557129, accuracy 0.29675\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 0, loss 0.055909112095832825, accuracy 0.3415\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 5] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 0.75, loss -3.5054268836975098, accuracy 0.338\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.06594626605510712, accuracy 0.2145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693384647369385, accuracy 0.1065\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.05845906585454941, accuracy 0.29575\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.25, loss 0.04212288558483124, accuracy 0.30125\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.5, loss 0.02568843401968479, accuracy 0.295\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 6] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.75, loss 0.009022976271808147, accuracy 0.2975\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.06506194174289703, accuracy 0.216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.05683913826942444, accuracy 0.3365\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 0.5, loss 0.024729713797569275, accuracy 0.3215\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 1.0, loss -0.00754344230517745, accuracy 0.10875\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 5, L 0.25, loss -0.004053746350109577, accuracy 0.32725\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 5, L 0.75, loss -0.12660951912403107, accuracy 0.33325\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 0, loss 0.0567537397146225, accuracy 0.33875\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 8] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.06611158698797226, accuracy 0.1955\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.3176698684692383, accuracy 0.333\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.25, loss 0.040532536804676056, accuracy 0.33125\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 9] fit, config: {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.06651607155799866, accuracy 0.21425\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.75, loss 0.008563226088881493, accuracy 0.33175\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0, loss 0.05661968141794205, accuracy 0.33125\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.5, loss -0.06551657617092133, accuracy 0.33325\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 1.0, loss -0.007543906103819609, accuracy 0.09875\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0, loss 0.05736340954899788, accuracy 0.3305\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.3177826404571533, accuracy 0.3285\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693384170532227, accuracy 0.103\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 1] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.3171942234039307, accuracy 0.32\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.0654481053352356, accuracy 0.2045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.6933913230896, accuracy 0.10425\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 2] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.06627749651670456, accuracy 0.202\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.75, loss 0.008727163076400757, accuracy 0.3175\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 0.25, loss 0.042268410325050354, accuracy 0.31675\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 0.75, loss 0.009007235057651997, accuracy 0.31125\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 5, L 0, loss 0.059074804186820984, accuracy 0.30775\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 5, L 0.5, loss -0.06449519842863083, accuracy 0.2975\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 5, L 1.0, loss -0.18788179755210876, accuracy 0.10725\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 4] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 0.25, loss -1.129197120666504, accuracy 0.308\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.06565018743276596, accuracy 0.2195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.056541167199611664, accuracy 0.34\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.5, loss 0.024453166872262955, accuracy 0.33075\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 7] fit, config: {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.06540113687515259, accuracy 0.208\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.75, loss 0.008665024302899837, accuracy 0.322\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 1.0, loss -0.007543465588241816, accuracy 0.0955\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 0.25, loss 0.04157496616244316, accuracy 0.3105\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.5, loss -0.0656038448214531, accuracy 0.33425\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 1.0, loss -0.18788090348243713, accuracy 0.093\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 5, L 0.25, loss -0.0032836010213941336, accuracy 0.3135\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.317957878112793, accuracy 0.33075\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.69338846206665, accuracy 0.09825\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.317337989807129, accuracy 0.31275\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 10 results and 0 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 0] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693378925323486, accuracy 0.10325\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 8] evaluate, config: {}\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 2] evaluate, config: {}\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No evaluate_metrics_aggregation_fn provided\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 1] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.059041280299425125, accuracy 0.30725\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 1] evaluate, config: {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.05484815314412117, accuracy 0.34775\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 2] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.0596264973282814, accuracy 0.298\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.5, loss 0.023776713758707047, accuracy 0.344\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 1.0, loss -0.007544628344476223, accuracy 0.11425\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.25, loss -0.005308089777827263, accuracy 0.3495\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.75, loss -0.12700796127319336, accuracy 0.34925\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0, loss 0.05480000749230385, accuracy 0.35675\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.3187010288238525, accuracy 0.3495\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693421840667725, accuracy 0.114\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 4] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.05997998267412186, accuracy 0.30125\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693416595458984, accuracy 0.1095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.05495108664035797, accuracy 0.3595\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 5] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.059663061052560806, accuracy 0.28575\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 0.5, loss 0.02382972277700901, accuracy 0.34475\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 1.0, loss -0.007543805055320263, accuracy 0.0995\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.25, loss -0.005512667819857597, accuracy 0.35125\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.75, loss -0.12698231637477875, accuracy 0.35475\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0, loss 0.05585779622197151, accuracy 0.34425\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.3186023235321045, accuracy 0.35175\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 0.75, loss -3.5056099891662598, accuracy 0.344\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 8] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693411350250244, accuracy 0.11\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.05907098948955536, accuracy 0.311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 9] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.053964875638484955, accuracy 0.358\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.05456224083900452, accuracy 0.366\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.05947379767894745, accuracy 0.29875\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.5, loss 0.023429252207279205, accuracy 0.35925\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 1.0, loss -0.0075442614033818245, accuracy 0.1025\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.25, loss -0.00606132997199893, accuracy 0.35375\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.75, loss -0.12721633911132812, accuracy 0.352\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0, loss 0.05393602326512337, accuracy 0.36325\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.319230079650879, accuracy 0.3545\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.6934075355529785, accuracy 0.10275\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 0] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.05927649512887001, accuracy 0.29875\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.6934075355529785, accuracy 0.102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.05485102906823158, accuracy 0.3475\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 3] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.05837594345211983, accuracy 0.323\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.5, loss 0.02353358268737793, accuracy 0.3625\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 1.0, loss -0.007544028572738171, accuracy 0.1005\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.25, loss -0.0060125007294118404, accuracy 0.35975\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.75, loss -0.12708067893981934, accuracy 0.354\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0, loss 0.05478782206773758, accuracy 0.346\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.3194732666015625, accuracy 0.3635\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693408966064453, accuracy 0.10725\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 7] fit, config: {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.0590386688709259, accuracy 0.29675\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693410873413086, accuracy 0.10475\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.05490326136350632, accuracy 0.34575\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 6] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.058878738433122635, accuracy 0.30725\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 0.5, loss 0.0237114317715168, accuracy 0.35025\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 1.0, loss -0.007544027175754309, accuracy 0.104\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 5, L 0.25, loss -0.006242481060326099, accuracy 0.35625\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 5, L 0.75, loss -0.12711432576179504, accuracy 0.355\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 0, loss 0.05462360009551048, accuracy 0.34975\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.319025993347168, accuracy 0.35975\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 10 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693409442901611, accuracy 0.11225\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 0] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.6934123039245605, accuracy 0.107\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 7] evaluate, config: {}\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 5] evaluate, config: {}\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 0] fit, config: {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.05538473278284073, accuracy 0.34975\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 4] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.052585817873477936, accuracy 0.37575\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 1] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.05576362460851669, accuracy 0.3415\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.5, loss 0.02236875519156456, accuracy 0.3775\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 1.0, loss -0.007544446736574173, accuracy 0.117\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.25, loss -0.00717960624024272, accuracy 0.37775\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.75, loss -0.12758107483386993, accuracy 0.37775\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0, loss 0.0524897575378418, accuracy 0.3745\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.320085048675537, accuracy 0.38125\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693417549133301, accuracy 0.1205\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 4] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.055780693888664246, accuracy 0.354\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693411350250244, accuracy 0.10075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.05226738378405571, accuracy 0.38675\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 5] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.055301420390605927, accuracy 0.336\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.5, loss 0.02247873693704605, accuracy 0.385\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 1.0, loss -0.007544389460235834, accuracy 0.09925\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.25, loss -0.007675366476178169, accuracy 0.384\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.75, loss -0.1276724934577942, accuracy 0.387\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0, loss 0.05224009230732918, accuracy 0.38925\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.3202431201934814, accuracy 0.38925\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693414688110352, accuracy 0.099\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 7] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.05511891096830368, accuracy 0.3485\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693422794342041, accuracy 0.1095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.05219300836324692, accuracy 0.37625\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 9] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.055137474089860916, accuracy 0.36325\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 0.25, loss 0.03673001378774643, accuracy 0.39575\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.75, loss 0.007355918176472187, accuracy 0.38575\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0, loss 0.05204752832651138, accuracy 0.3895\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 5, L 0.5, loss -0.06783019751310349, accuracy 0.383\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 5, L 0.75, loss -0.127930149435997, accuracy 0.3985\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0, loss 0.05170214921236038, accuracy 0.39325\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 0.5, loss -2.3204026222229004, accuracy 0.37975\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.693415641784668, accuracy 0.10375\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m [Client 2] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Epoch 1: train loss 0.05549490824341774, accuracy 0.35575\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 25, L 1.0, loss -4.6934123039245605, accuracy 0.09975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m [Client 6] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0, loss 0.05223279073834419, accuracy 0.384\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Epoch 1: train loss 0.054181091487407684, accuracy 0.358\n",
            "\u001b[36m(ClientAppActor pid=7784)\u001b[0m Distillation Epoch 1: T 1, L 0.5, loss 0.02225659415125847, accuracy 0.3935\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7783)\u001b[0m Distillation Epoch 1: T 1, L 0.75, loss 0.0071329157799482346, accuracy 0.397\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "NUM_PARTITIONS = 10\n",
        "\n",
        "\n",
        "def server_fn(context: Context) -> ServerAppComponents:\n",
        "    # Configure the server for just 3 rounds of training\n",
        "    config = ServerConfig(num_rounds=3)\n",
        "    # If no strategy is provided, by default, ServerAppComponents will use FedAvg\n",
        "    return ServerAppComponents(config=config)\n",
        "\n",
        "\n",
        "# Create the ServerApp\n",
        "server = ServerApp(server_fn=server_fn)\n",
        "\n",
        "# Specify the resources each of your clients need\n",
        "# If set to none, by default, each client will be allocated 2x CPU and 0x GPUs\n",
        "backend_config = {\"client_resources\": None}\n",
        "if DEVICE.type == \"cuda\":\n",
        "    backend_config = {\"client_resources\": {\"num_gpus\": 0.1,\"num_cpus\": 1}}\n",
        "\n",
        "# Run simulation\n",
        "run_simulation(\n",
        "    server_app=server,\n",
        "    client_app=client,\n",
        "    num_supernodes=NUM_PARTITIONS,\n",
        "    backend_config=backend_config,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}